from __future__ import annotations

import re
from typing import Dict, List, Tuple, Optional

import pandas as pd

from executor import execute_code
from llm import llm_autonomous_eda_code, llm_generate_key_findings


def _remove_duplicate_sentences(text: str) -> str:
    """Remove duplicate sentences from text, keeping only the first occurrence."""
    if not text or not text.strip():
        return text
    
    lines = text.split('\n')
    seen_sentences = set()
    cleaned_lines = []
    
    for line in lines:
        line = line.strip()
        if not line:
            cleaned_lines.append('')
            continue
            
        # Check if this line is a duplicate sentence
        # Normalize the sentence for comparison (remove extra spaces, convert to lowercase)
        normalized = re.sub(r'\s+', ' ', line.lower().strip())
        
        if normalized in seen_sentences:
            # Skip duplicate sentence
            continue
        else:
            seen_sentences.add(normalized)
            cleaned_lines.append(line)
    
    return '\n'.join(cleaned_lines)


SECTION_HEADERS = [
    "Dataset Summary",
    "Visualizations",
    "Statistical Insights",
    "ML Results",
    "Final Explanation",
]


def compute_dataset_info(df: pd.DataFrame) -> Tuple[Dict[str, str], Dict[str, float], Dict[str, int]]:
    schema = {col: str(dtype) for col, dtype in df.dtypes.items()}
    missing_pct = {col: float(df[col].isna().mean() * 100.0) for col in df.columns}
    distinct_counts = {col: int(df[col].nunique(dropna=False)) for col in df.columns}
    return schema, missing_pct, distinct_counts


def extract_code(text: str) -> str:
    # If the model returns code fences, extract; else return as-is
    m = re.search(r"```(?:python)?\n([\s\S]*?)```", text)
    if m:
        return m.group(1).strip()
    return text.strip()


def convert_pandas_to_tabulate(stdout: str) -> str:
    """Convert pandas table output to tabulate format for better display."""
    import re
    
    # Look for pandas-style table patterns and convert them
    lines = stdout.split('\n')
    converted_lines = []
    
    i = 0
    while i < len(lines):
        line = lines[i]
        
        # Check if this looks like a pandas table header (contains column names)
        if any(col in line.lower() for col in ['popularity', 'acousticness', 'danceability', 'energy', 'count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']):
            # This might be a pandas table - look ahead to see if it's a full table
            table_lines = [line]
            j = i + 1
            
            # Collect subsequent lines that look like table data
            while j < len(lines) and (any(char.isdigit() for char in lines[j]) and lines[j].count('  ') >= 2):
                table_lines.append(lines[j])
                j += 1
            
            if len(table_lines) >= 2:  # At least header + one data row
                # Convert this table section to tabulate format
                converted_lines.append("```")
                converted_lines.extend(table_lines)
                converted_lines.append("```")
                i = j - 1  # Skip the lines we just processed
            else:
                converted_lines.append(line)
        else:
            converted_lines.append(line)
        
        i += 1
    
    return '\n'.join(converted_lines)


def auto_convert_to_tabulate(stdout: str) -> str:
    """Automatically convert pandas-style table output to tabulate format."""
    import re
    
    # Split into lines and process
    lines = stdout.split('\n')
    result_lines = []
    
    i = 0
    while i < len(lines):
        line = lines[i]
        
        # Look for pandas table patterns
        if any(term in line.lower() for term in ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max', 'popularity', 'acousticness', 'danceability']):
            # This might be a table header - collect the table
            table_lines = [line]
            j = i + 1
            
            # Collect subsequent lines that look like table data
            while j < len(lines):
                next_line = lines[j]
                # Check if this line contains numeric data with proper spacing
                if (any(char.isdigit() for char in next_line) and 
                    next_line.count('  ') >= 2 and 
                    len(next_line.strip()) > 10):
                    table_lines.append(next_line)
                    j += 1
                else:
                    break
            
            if len(table_lines) >= 2:
                # Convert to tabulate format
                result_lines.append("```")
                result_lines.extend(table_lines)
                result_lines.append("```")
                i = j - 1
            else:
                result_lines.append(line)
        else:
            result_lines.append(line)
        
        i += 1
    
    return '\n'.join(result_lines)


def format_pandas_output(stdout: str) -> str:
    """Post-process stdout to format raw pandas output for better readability."""
    # Add section separators if missing
    if "### SECTION:" in stdout and "=" not in stdout:
        stdout = stdout.replace("### SECTION:", "\n" + "="*60 + "\n### SECTION:")
    
    # Simple approach: wrap pandas tables in code blocks for better display
    lines = stdout.split('\n')
    formatted_lines = []
    
    i = 0
    while i < len(lines):
        line = lines[i]
        
        # Check if this looks like a pandas table header
        if any(term in line.lower() for term in ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']):
            # Start a code block
            formatted_lines.append("```")
            formatted_lines.append(line)
            
            # Continue adding lines until we hit a non-table line
            j = i + 1
            while j < len(lines):
                next_line = lines[j]
                if (any(char.isdigit() for char in next_line) and 
                    next_line.count('  ') >= 2):
                    formatted_lines.append(next_line)
                    j += 1
                else:
                    break
            
            # End the code block
            formatted_lines.append("```")
            i = j - 1
        else:
            formatted_lines.append(line)
        
        i += 1
    
    return '\n'.join(formatted_lines)


def parse_sections(stdout: str) -> Dict[str, str]:
    pattern = re.compile(r"^### SECTION:\s*(.+)\s*$", re.MULTILINE)
    parts: Dict[str, str] = {h: "" for h in SECTION_HEADERS}
    indices = [(m.start(), m.group(1).strip()) for m in pattern.finditer(stdout)]
    
    # DEBUG: Print parsing info
    print(f"DEBUG: parse_sections - stdout length: {len(stdout)}")
    print(f"DEBUG: parse_sections - found {len(indices)} section markers")
    for start, title in indices:
        print(f"DEBUG: parse_sections - found marker: '{title}' at position {start}")
    
    if not indices:
        # Fallback: try to intelligently parse content without section markers
        print(f"DEBUG: parse_sections - no markers found, trying intelligent parsing")
        
        # Look for common patterns to identify sections
        lines = stdout.split('\n')
        current_section = "Dataset Summary"
        section_content = {h: [] for h in SECTION_HEADERS}
        
        for line in lines:
            line_lower = line.lower()
            
            # Try to identify sections based on content
            if any(term in line_lower for term in ['key findings', 'summary', 'conclusion', 'final', 'explanation']):
                current_section = "Final Explanation"
            elif any(term in line_lower for term in ['correlation', 'statistical', 'insights', 'analysis']):
                current_section = "Statistical Insights"
            elif any(term in line_lower for term in ['visualization', 'plot', 'chart', 'figure']):
                current_section = "Visualizations"
            elif any(term in line_lower for term in ['ml', 'model', 'prediction', 'training']):
                current_section = "ML Results"
            elif any(term in line_lower for term in ['dataset', 'shape', 'dtype', 'missing']):
                current_section = "Dataset Summary"
            
            section_content[current_section].append(line)
        
        # Join content for each section
        for section in SECTION_HEADERS:
            if section_content[section]:
                content = '\n'.join(section_content[section]).strip()
                
                # Remove markdown code blocks from the content
                content = re.sub(r'```[a-zA-Z]*\n', '', content)  # Remove ```python, ```, etc.
                content = re.sub(r'\n```\n', '\n', content)  # Remove closing ```
                content = re.sub(r'^```\n', '', content)  # Remove opening ``` at start
                content = re.sub(r'\n```$', '', content)  # Remove closing ``` at end
                content = re.sub(r'^```$', '', content)  # Remove standalone ```
                content = re.sub(r'```$', '', content)  # Remove ``` at end of line
                
                parts[section] = content.strip()
        
        return parts
    indices.append((len(stdout), "__END__"))
    for i in range(len(indices) - 1):
        start, title = indices[i]
        end, _ = indices[i + 1]
        # Skip the header line itself
        header_line_end = stdout.find("\n", start)
        body_start = header_line_end + 1 if header_line_end != -1 else start
        content = stdout[body_start:end].strip()
        # Normalize title to our known headers if possible
        for h in SECTION_HEADERS:
            if h.lower() == title.lower():
                # Format the content for better readability
                formatted_content = format_pandas_output(content)
                
                # Remove markdown code blocks from the content
                formatted_content = re.sub(r'```[a-zA-Z]*\n', '', formatted_content)  # Remove ```python, ```, etc.
                formatted_content = re.sub(r'\n```\n', '\n', formatted_content)  # Remove closing ```
                formatted_content = re.sub(r'^```\n', '', formatted_content)  # Remove opening ``` at start
                formatted_content = re.sub(r'\n```$', '', formatted_content)  # Remove closing ``` at end
                formatted_content = re.sub(r'^```$', '', formatted_content)  # Remove standalone ```
                formatted_content = re.sub(r'```$', '', formatted_content)  # Remove ``` at end of line
                
                parts[h] = formatted_content.strip()
                print(f"DEBUG: parse_sections - assigned '{title}' to '{h}' with {len(formatted_content)} chars")
                break
    return parts


def _fallback_script(df: pd.DataFrame, topic: str = None, time_budget_min: int = 5) -> str:
    """Return a simple, deterministic EDA script with required sections.

    Uses pandas/numpy/matplotlib/seaborn only, avoids plt.show/close, and limits plots.
    """
    return (
        "import pandas as pd\n"
        "import numpy as np\n"
        "import matplotlib.pyplot as plt\n"
        "import seaborn as sns\n\n"
        "print('### SECTION: Dataset Summary')\n"
        "print('Shape:', df.shape)\n"
        "print('Dtypes:')\n"
        "print(df.dtypes)\n"
        "print('\\nMissing % by column:')\n"
        "print((df.isnull().mean()*100).round(2))\n\n"
        "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n"
        "cat_cols = df.select_dtypes(include=['object','category','bool']).columns.tolist()\n\n"
        "print('### SECTION: Visualizations')\n"
        "plotted = 0\n"
        "# Up to 3 numeric histograms\n"
        "for col in num_cols[:3]:\n"
        "    fig, ax = plt.subplots(figsize=(6,3))\n"
        "    ax.hist(df[col].dropna(), bins=30, color='steelblue', edgecolor='white')\n"
        "    ax.set_title(f'Distribution of {col}')\n"
        "    ax.set_xlabel(col)\n"
        "    ax.set_ylabel('Frequency')\n"
        "    plotted += 1\n"
        "# One categorical countplot\n"
        "if cat_cols:\n"
        "    col = cat_cols[0]\n"
        "    vc = df[col].astype(str).value_counts().head(12)\n"
        "    fig, ax = plt.subplots(figsize=(6,3))\n"
        "    sns.barplot(x=vc.index, y=vc.values, ax=ax)\n"
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n"
        "    ax.set_title(f'Counts of {col} (top 12)')\n"
        "    ax.set_xlabel(col)\n"
        "    ax.set_ylabel('Count')\n\n"
        "print('### SECTION: Statistical Insights')\n"
        "if num_cols:\n"
        "    desc = df[num_cols].describe().T\n"
        "    print(desc)\n"
        "    if len(num_cols) >= 2:\n"
        "        corr = df[num_cols].corr(numeric_only=True)\n"
        "        fig, ax = plt.subplots(figsize=(min(10, 0.6*corr.shape[1]+3), min(8, 0.6*corr.shape[0]+3)))\n"
        "        cax = ax.imshow(corr.values, cmap='coolwarm', vmin=-1, vmax=1)\n"
        "        ax.set_xticks(range(corr.shape[1]))\n"
        "        ax.set_xticklabels(corr.columns, rotation=90)\n"
        "        ax.set_yticks(range(corr.shape[0]))\n"
        "        ax.set_yticklabels(corr.index)\n"
        "        fig.colorbar(cax, ax=ax, fraction=0.046, pad=0.04)\n"
        "        ax.set_title('Correlation heatmap')\n\n"
        "print('### SECTION: ML Results')\n"
        "print('Skipped in fallback to ensure robustness.')\n\n"
        "print('### SECTION: Final Explanation')\n"
        "print('This fallback analysis summarizes structure, missingness, basic distributions, and correlations to provide quick insights when the autonomous agent encounters an error.')\n"
    )


def _last_resort_sections(df: pd.DataFrame) -> Dict[str, str]:
    schema = {col: str(dtype) for col, dtype in df.dtypes.items()}
    missing = (df.isnull().mean() * 100).round(2).to_string()
    summary = [
        f"Shape: {df.shape}",
        "Dtypes:",
        pd.Series(schema).to_string(),
        "",
        "Missing % by column:",
        missing,
    ]
    sections = {h: "" for h in SECTION_HEADERS}
    sections["Dataset Summary"] = "\n".join(summary)
    sections["Final Explanation"] = (
        "Minimal summary generated due to repeated execution errors."
    )
    return sections


def sanitize_code(code: str, df: pd.DataFrame = None, time_budget_min: int = None) -> str:
    """Simplified sanitizer that only does essential fixes without breaking the code."""
    # Remove plt.show/close to allow capture
    code = re.sub(r"plt\s*\.\s*show\s*\(\s*\)", "", code)
    code = re.sub(r"plt\s*\.\s*close\s*\(\s*(['\"])all\1\s*\)", "", code)

    # Remove import streamlit statements since we provide a mock
    code = re.sub(r"import\s+streamlit\s*", "", code)
    code = re.sub(r"from\s+streamlit\s+import\s+.*\n", "", code)
    
    # Remove markdown code blocks
    code = re.sub(r'```[a-zA-Z]*\n', '', code)
    code = re.sub(r'\n```\n', '\n', code)
    
    # Fix seaborn boxplot issues with sampling - ensure sufficient data per category
    code = re.sub(
        r"sns\.boxplot\(data=([^,]+),\s*x='([^']+)',\s*y='([^']+)'\)",
        r"# Filter data to ensure sufficient points per category for boxplot\n    boxplot_data = \1.groupby('\2').filter(lambda x: len(x) >= 3)\n    if len(boxplot_data) > 0:\n        sns.boxplot(data=boxplot_data, x='\2', y='\3')\n    else:\n        plt.text(0.5, 0.5, 'Boxplot skipped - insufficient data per category', ha='center', va='center', transform=plt.gca().transAxes)",
        code
    )
    
    # Add try/except around problematic seaborn operations to prevent crashes
    code = re.sub(
        r"(sns\.boxplot\([^)]+\))",
        r"try:\n    \1\nexcept (ValueError, IndexError) as e:\n    print(f'Boxplot skipped due to data issue: {e}')\n    plt.text(0.5, 0.5, 'Boxplot skipped - insufficient data', ha='center', va='center', transform=plt.gca().transAxes)",
        code
    )

    return code


def run_autonomous_session(
    df: pd.DataFrame,
    goal: str,
    time_budget_min: int,
    model: str,
    timeout_seconds: int,
    topic: Optional[str] = None,
    hide_on_error: bool = True,
) -> Dict[str, object]:
    schema, missing_pct, distinct_counts = compute_dataset_info(df)

    # bound LLM codegen time to 20% of budget (min 5s, max budget-10s)
    llm_timeout = max(5, min(int(0.2 * timeout_seconds), max(5, timeout_seconds - 10)))
    
    print(f"DEBUG: Attempting LLM call with timeout {llm_timeout}s")
    try:
        code_text = llm_autonomous_eda_code(
            schema,
            missing_pct,
            distinct_counts,
            goal,
            time_budget_min,
            model=model,
            topic=topic,
            request_timeout=llm_timeout,
        )
        print(f"DEBUG: LLM call successful, response length: {len(code_text)}")
        original_code = extract_code(code_text)
        print(f"DEBUG: Extracted original code length: {len(original_code)}")
        code = sanitize_code(original_code, df, time_budget_min)
        print(f"DEBUG: Sanitized code length: {len(code)}")
        
        # Try to compile the code to catch any syntax errors
        try:
            compile(code, "<string>", "exec")
            print(f"DEBUG: Code compiles successfully")
        except SyntaxError as se:
            print(f"DEBUG: Syntax error: {se}")
            # Skip aggressive syntax fixing to avoid breaking LLM code
            pass
    code = re.sub(
        r"grouped\.columns\s*=\s*\[([^\]]+)\]",
        r"grouped.columns = [\1]  # Fixed groupby columns",
        code
    )
    
    # Fix groupby operations that create MultiIndex columns by using dict instead of list
    code = re.sub(
        r"\.agg\(\[([^\]]+)\]\)\.reset_index\(\)",
        r".agg({\1}).reset_index()",
        code
    )
    
    # Fix the specific groupby issue: when agg(['count', 'mean']) creates MultiIndex columns
    # but code tries to assign simple column names
    code = re.sub(
        r"(\w+)\.agg\(\[([^\]]+)\]\)\.reset_index\(\)\s*\n\s*\1\.columns\s*=\s*\[([^\]]+)\]",
        r"\1.agg({\2}).reset_index()\n\1.columns = [\3]  # Fixed MultiIndex columns",
        code,
        flags=re.MULTILINE
    )
    
    # Fix seaborn boxplot issues with sampling - ensure sufficient data per category
    code = re.sub(
        r"sns\.boxplot\(data=([^,]+),\s*x='([^']+)',\s*y='([^']+)'\)",
        r"# Filter data to ensure sufficient points per category for boxplot\n    boxplot_data = \1.groupby('\2').filter(lambda x: len(x) >= 3)\n    if len(boxplot_data) > 0:\n        sns.boxplot(data=boxplot_data, x='\2', y='\3')\n    else:\n        plt.text(0.5, 0.5, 'Boxplot skipped - insufficient data per category', ha='center', va='center', transform=plt.gca().transAxes)",
        code
    )
    
    # Add try/except around problematic seaborn operations to prevent crashes
    code = re.sub(
        r"(sns\.boxplot\([^)]+\))",
        r"try:\n    \1\nexcept (ValueError, IndexError) as e:\n    print(f'Boxplot skipped due to data issue: {e}')\n    plt.text(0.5, 0.5, 'Boxplot skipped - insufficient data', ha='center', va='center', transform=plt.gca().transAxes)",
        code
    )

    # Fix KeyError issues by replacing references to non-existent columns
    # Get actual column names from the DataFrame
    if df is not None:
        actual_columns = list(df.columns)
        print(f"DEBUG: Actual columns in DataFrame: {actual_columns}")
    else:
        # If df is not provided, skip column mapping
        actual_columns = []
    
    # Common column name mappings for different datasets
    column_mappings = {
        'genre': ['genre', 'category', 'type', 'class'],
        'artist_name': ['artist_name', 'artist', 'performer', 'creator'],
        'track_name': ['track_name', 'title', 'name', 'song'],
        'popularity': ['popularity', 'rating', 'score'],
        'acousticness': ['acousticness', 'acoustic'],
        'danceability': ['danceability', 'dance'],
        'energy': ['energy', 'energetic'],
        'valence': ['valence', 'positivity', 'mood'],
        'tempo': ['tempo', 'bpm', 'speed'],
        'loudness': ['loudness', 'volume'],
        'duration_ms': ['duration_ms', 'duration', 'length', 'time'],
        'instrumentalness': ['instrumentalness', 'instrumental'],
        'liveness': ['liveness', 'live'],
        'speechiness': ['speechiness', 'speech'],
        'key': ['key', 'musical_key'],
        'mode': ['mode', 'musical_mode'],
        'time_signature': ['time_signature', 'time_sig'],
    }
    
    # Replace references to non-existent columns with safe alternatives
    # DISABLED - causing issues with valid column names in multiprocessing
    # for assumed_col, possible_names in column_mappings.items():
    #     # Check if the column exists (either as assumed_col or any of its possible names)
    #     # Use case-insensitive comparison
    #     actual_columns_lower = [col.lower() for col in actual_columns]
    #     column_exists = assumed_col.lower() in actual_columns_lower
    #     
    #     if not column_exists:
    #         # Check if any of the possible names exist
    #         for possible_name in possible_names:
    #             if possible_name.lower() in actual_columns_lower:
    #                 column_exists = True
    #                 break
    #     
    #     if not column_exists:
    #         # Find a suitable replacement
    #         replacement = None
    #         for possible_name in possible_names:
    #             if possible_name.lower() in actual_columns_lower:
    #                 # Find the actual column name with correct case
    #                 for actual_col in actual_columns:
    #                     if actual_col.lower() == possible_name.lower():
    #                         replacement = actual_col
    #                         break
    #                 if replacement:
    #                     break
    #         
    #         if replacement:
    #             print(f"DEBUG: Replacing '{assumed_col}' with '{replacement}'")
    #             # Replace the column reference
    #             code = re.sub(
    #                 rf"df\[['\"]{assumed_col}['\"]\]",
    #                 f"df['{replacement}']",
    #                 code
    #             )
    #             code = re.sub(
    #                 rf"df\.{assumed_col}",
    #                 f"df['{replacement}']",
    #                 code
    #             )
    #         else:
    #             print(f"DEBUG: Column '{assumed_col}' not found, commenting out lines")
    #             # If no replacement found, comment out the problematic line
    #             code = re.sub(
    #                 rf"([^#]*df\[['\"]{assumed_col}['\"]\][^;\n]*;)",
    #                 r"# \1  # Column not found - skipping",
    #                 code
    #             )
    #             code = re.sub(
    #                 rf"([^#]*df\.{assumed_col}[^;\n]*;)",
    #                 r"# \1  # Column not found - skipping",
    #                 code
    #             )

    # More comprehensive fix: catch any problematic DataFrame assignment
    # This looks for patterns like df['column'] = some_expression that might return a DataFrame
    def fix_dataframe_assignment(match):
        assignment = match.group(0)
        # Check if the right side might return a DataFrame
        right_side = match.group(1)
        if any(method in right_side for method in ['.describe()', '.corr()', '.groupby(', '.agg(', '.pivot_table(', '.crosstab(']):
            return f"# Fixed: {assignment}  # This would cause ValueError - skipping"
        return assignment

    # Apply the fix to all DataFrame assignments
    code = re.sub(
        r"df\[['\"]([^'\"]+)['\"]\]\s*=\s*([^;\n]+)",
        fix_dataframe_assignment,
        code
    )

    # Convert pandas to_string() calls to tabulate for better formatting
    code = re.sub(r"\.to_string\(\)", ".to_string()", code)  # Keep original for now
    # Add tabulate import if not present
    if "import tabulate" not in code and "from tabulate import" not in code:
        code = "from tabulate import tabulate\n" + code

    # Replace pandas print statements with tabulate
    code = re.sub(
        r"print\s*\(\s*['\"]\s*([^'\"]+)\s*['\"]\s*\+\s*([^+]+)\.to_string\(\)\s*\)",
        r"print('\1' + tabulate(\2, headers='keys', tablefmt='grid', showindex=True))",
        code
    )
    code = re.sub(
        r"print\s*\(\s*([^+]+)\.to_string\(\)\s*\)",
        r"print(tabulate(\1, headers='keys', tablefmt='grid', showindex=True))",
        code
    )

    # Convert print() statements with DataFrames to st.dataframe() calls
    code = re.sub(
        r"print\s*\(\s*df\.describe\(\)\s*\)",
        r"st.dataframe(df.describe().round(3), use_container_width=False, hide_index=True)",
        code
    )
    code = re.sub(
        r"print\s*\(\s*df\.info\(\)\s*\)",
        r"st.dataframe(df.info(), use_container_width=False, hide_index=True)",
        code
    )
    code = re.sub(
        r"print\s*\(\s*df\.head\(\)\s*\)",
        r"st.dataframe(df.head(), use_container_width=False, hide_index=True)",
        code
    )
    code = re.sub(
        r"print\s*\(\s*df\.isnull\(\)\.sum\(\)\s*\)",
        r"st.dataframe(df.isnull().sum().to_frame('Missing Count'), use_container_width=False, hide_index=True)",
        code
    )
    code = re.sub(
        r"print\s*\(\s*df\.dtypes\s*\)",
        r"st.dataframe(df.dtypes.to_frame('dtype'), use_container_width=False, hide_index=True)",
        code
    )

    # Remove markdown code blocks from the entire code
    code = re.sub(r'```[a-zA-Z]*\n', '', code)  # Remove ```python, ```, etc.
    code = re.sub(r'\n```\n', '\n', code)  # Remove closing ```
    
    # Remove import streamlit statements since we provide a mock
    code = re.sub(r"import\s+streamlit\s*", "", code)
    code = re.sub(r"from\s+streamlit\s+import\s+.*\n", "", code)
    
    # Aggressively remove any custom _St class definitions and assignments
    # Remove the entire class definition and assignment
    code = re.sub(
        r"class\s+_St:.*?st\s*=\s*_St\(\)",
        "# Custom _St class removed - using provided mock\nst = st  # Use the provided mock",
        code,
        flags=re.DOTALL
    )
    
    # Remove just the class definition if assignment is on separate lines
    code = re.sub(
        r"class\s+_St:.*?(?=\n\s*st\s*=\s*_St\(\))",
        "# Custom _St class removed - using provided mock",
        code,
        flags=re.DOTALL
    )
    
    # Remove any st = _St() assignments
    code = re.sub(r"st\s*=\s*_St\(\)", "st = st  # Use the provided mock", code)
    
    # Remove any remaining _St class definitions
    code = re.sub(
        r"class\s+_St:.*?(?=\n\s*[^#\s])",
        "# Custom _St class removed - using provided mock",
        code,
        flags=re.DOTALL
    )

    # Fix undefined variable references
    code = re.sub(r"\btopic\b(?!\s*=)", '"general analysis"', code)  # Replace undefined topic with string
    code = re.sub(r"\bgoal\b(?!\s*=)", '"analyze data"', code)  # Replace undefined goal with string
    code = re.sub(r"\bfocus\b(?!\s*=)", '"data analysis"', code)  # Replace undefined focus with string
    code = re.sub(r"\bcontext\b(?!\s*=)", '"general analysis"', code)  # Replace undefined context with string
    
    # Fix common LLM mistakes
    code = re.sub(r"df\.describe\(\)\.describe\(\)", "df.describe()", code)  # Double describe
    code = re.sub(r"df\.corr\(\)\.corr\(\)", "df.corr()", code)  # Double corr
    code = re.sub(r"df\.head\(\)\.head\(\)", "df.head()", code)  # Double head
    
    # Fix duplicate column names in value_counts().reset_index() operations
    # This pattern: df['col'].value_counts().reset_index().rename(columns={'index':'col','col':'count'})
    # Can create duplicate 'count' columns if not handled properly
    code = re.sub(
        r"\.rename\(columns=\{'index':([^,]+),([^}]+):'count'\}\)",
        r".rename(columns={'index':\1,\2:'count'})",
        code
    )
    
    # Fix the specific pattern that causes duplicate 'count' columns
    # Pattern: .rename(columns={'index':'artist_name','artist_name':'count'})
    # This creates both 'artist_name' and 'count' columns, but if there's already a 'count' column, it fails
    code = re.sub(
        r"\.rename\(columns=\{'index':'([^']+)','\1':'count'\}\)",
        r".rename(columns={'index':'\1','\1':'count'})",
        code
    )
    
    # Alternative fix: ensure unique column names in rename operations
    code = re.sub(
        r"\.rename\(columns=\{'index':([^,]+),([^}]+):'count'\}\)",
        r".rename(columns={'index':\1,\2:'count_value'})",
        code
    )
    
    # Fix sklearn parameter issues
    code = re.sub(r"squared=True", "", code)  # Remove squared=True parameter
    code = re.sub(r"squared=False", "", code)  # Remove squared=False parameter
    code = re.sub(r",\s*squared=[^,)]+", "", code)  # Remove squared parameter with any value
    
    # Fix other common sklearn parameter issues
    code = re.sub(r"null_counts=True", "show_counts=True", code)  # Fix DataFrame.info() parameter
    code = re.sub(r"null_counts=False", "show_counts=False", code)  # Fix DataFrame.info() parameter
    
    # Fix sklearn target variable shape issues - use non-capturing groups
    code = re.sub(r"y\s*=\s*\([^)]+\)\.astype\(int\)", r"y = \g<0>.values.ravel()", code)  # Ensure 1D array
    code = re.sub(r"y\s*=\s*\([^)]+\)\.astype\(int\)\.values", r"y = \g<0>.values.ravel()", code)  # Ensure 1D array
    
    # Fix common target variable patterns - use non-capturing groups
    code = re.sub(r"y\s*=\s*\([^)]+popularity[^)]+median[^)]*\)\.astype\(int\)", r"y = \g<0>.values.ravel()", code)
    code = re.sub(r"y\s*=\s*\([^)]+>=.*median[^)]*\)\.astype\(int\)", r"y = \g<0>.values.ravel()", code)
    
    # Add .ravel() to any target variable assignment to ensure 1D array
    code = re.sub(r"(y\s*=\s*[^;\n]+\.astype\(int\))(?!\s*\.ravel\(\))", r"\1.ravel()", code)
    
    # For 5-minute budgets, remove ML-related code
    if time_budget_min == 5:
        # Remove ML-related imports
        code = re.sub(r"from sklearn\.model_selection import.*\n", "", code)
        code = re.sub(r"from sklearn\.linear_model import.*\n", "", code)
        code = re.sub(r"from sklearn\.tree import.*\n", "", code)
        code = re.sub(r"from sklearn\.metrics import.*\n", "", code)
        
        # Remove ML-related code blocks - use non-capturing groups
        code = re.sub(r"# SECTION: ML Results.*?# SECTION: Final Explanation", "# SECTION: Final Explanation", code, flags=re.DOTALL)
        code = re.sub(r"# ML Results.*?# Final Explanation", "# Final Explanation", code, flags=re.DOTALL)
        
        # Remove specific ML operations - use non-capturing groups
        code = re.sub(r"train_test_split\([^)]*\)", "# train_test_split removed for 5min budget", code)
        code = re.sub(r"LogisticRegression\([^)]*\)", "# LogisticRegression removed for 5min budget", code)
        code = re.sub(r"DecisionTreeClassifier\([^)]*\)", "# DecisionTreeClassifier removed for 5min budget", code)
        code = re.sub(r"confusion_matrix\([^)]*\)", "# confusion_matrix removed for 5min budget", code)
        
        # Remove plot loops that could cause performance issues
        code = re.sub(r"for\s+\w+\s+in\s+\w+:\s*\n\s*plt\.figure\([^)]*\)", "# Plot loop removed for performance", code, flags=re.MULTILINE)
        code = re.sub(r"for\s+\w+\s+in\s+\w+:\s*\n\s*sns\.[^)]*\)", "# Seaborn plot loop removed for performance", code, flags=re.MULTILINE)

    # No limits on st.dataframe calls - allow all for comprehensive display
    print("DEBUG: No st.dataframe limits applied - all calls allowed")

    # Fix multi-line SECTION header prints like: print('\n### SECTION: X')
    code = re.sub(
        r"print\(\s*([\"'])\s*[\r\n]+\s*(### SECTION:[^\"']+?)\s*\1\s*\)",
        r"print(\1\2\1)",
        code,
        flags=re.MULTILINE,
    )
    
    # Fix empty try blocks followed by except
    code = re.sub(
        r"try:\s*\nexcept\s+Exception:",
        r"try:\n    pass\nexcept Exception:",
        code,
        flags=re.MULTILINE,
    )

    # Fix common syntax errors in LLM-generated code
    # Fix missing parentheses in function calls
    code = re.sub(r"st\.dataframe\s+([^,]+)\s*,", r"st.dataframe(\1,", code)
    code = re.sub(r"st\.dataframe\s+([^)]+)\s*$", r"st.dataframe(\1)", code)
    
    # Fix missing quotes around string literals
    code = re.sub(r"st\.dataframe\(([^,]+),\s*use_container_width=([^,)]+)\)", r"st.dataframe(\1, use_container_width=\2)", code)
    code = re.sub(r"st\.dataframe\(([^,]+),\s*hide_index=([^,)]+)\)", r"st.dataframe(\1, hide_index=\2)", code)
    
    # Fix missing commas between arguments and make tables fit content
    code = re.sub(r"use_container_width=True\s*hide_index=True", r"use_container_width=False, hide_index=True", code)
    code = re.sub(r"hide_index=True\s*use_container_width=True", r"hide_index=True, use_container_width=False", code)
    # Change all use_container_width=True to False to make tables fit content
    code = re.sub(r"use_container_width=True", r"use_container_width=False", code)

    # Collapse generic multi-line string literals inside print('...') / print("...")
    code = re.sub(r"print\(\s*'\s*[\r\n]+([^']*)'\s*\)", r"print('\1')", code, flags=re.MULTILINE)
    code = re.sub(r'print\(\s*"\s*[\r\n]+([^"]*)"\s*\)', r'print("\1")', code, flags=re.MULTILINE)

    # Fix pattern: newline immediately after opening quote in print(
    code = re.sub(r"print\(\s*([\"'])\s*[\r\n]+\s*", r"print(\1", code)

    # Aggressively fix lines like: print('\n) followed by content on next line
    lines = code.splitlines()
    fixed: List[str] = []
    i = 0
    while i < len(lines):
        cur = lines[i]
        s = cur.strip()
        if s in ("print('", 'print("'):
            quote = '"' if '"' in s else "'"
            next_line = lines[i + 1].strip() if i + 1 < len(lines) else ""
            tail = next_line.rstrip().rstrip(')').rstrip('"').rstrip("'")
            tail_escaped = tail.replace(quote, f"\\{quote}")
            combined = f"print({quote}{tail_escaped}{quote})"
            fixed.append(combined)
            i += 2
            continue
        fixed.append(cur)
        i += 1
    code = "\n".join(fixed)

    # Final syntax check and fix
    try:
        compile(code, '<string>', 'exec')
    except SyntaxError as e:
        print(f"DEBUG: Syntax error detected: {e}")
        # Skip aggressive syntax fixing - it's breaking the code
        # Just return the original code and let the executor handle it
        print(f"DEBUG: Skipping syntax fixes to avoid breaking LLM-generated code")
        return code

    return code
            
            # Fix missing colons in if/for/while statements
            if any(keyword in line for keyword in ['if ', 'for ', 'while ', 'def ', 'class ', 'try:', 'except:', 'finally:', 'else:', 'elif ']) and not line.strip().endswith(':'):
                if not line.strip().endswith(':'):
                    line = line.rstrip() + ':'
            
            # Fix try/except blocks with missing indentation
            if line.strip() == 'try:':
                # Ensure the next line is properly indented
                if i + 1 < len(lines) and not lines[i + 1].strip().startswith(' ') and lines[i + 1].strip():
                    # Add indentation to the next line
                    lines[i + 1] = '    ' + lines[i + 1]
                # If try block is empty, add a pass statement
                elif i + 1 < len(lines) and lines[i + 1].strip() == 'except Exception:':
                    # Insert a pass statement after try:
                    lines.insert(i + 1, '    pass')
                    print(f"DEBUG: Added pass statement after empty try block")
            elif line.strip() == 'except:':
                # Ensure the next line is properly indented
                if i + 1 < len(lines) and not lines[i + 1].strip().startswith(' ') and lines[i + 1].strip():
                    # Add indentation to the next line
                    lines[i + 1] = '    ' + lines[i + 1]
            elif line.strip() == 'except Exception:':
                # Ensure the next line is properly indented
                if i + 1 < len(lines) and not lines[i + 1].strip().startswith(' ') and lines[i + 1].strip():
                    # Add indentation to the next line
                    lines[i + 1] = '    ' + lines[i + 1]
            
                # Fix incomplete import statements like "as st" -> "import streamlit as st"
            if line.strip() == 'as st':
                line = 'import streamlit as st'
                print(f"DEBUG: Fixed incomplete import 'as st' -> 'import streamlit as st'")
            
            # Fix mixed indentation issues early
            if line.strip() and line.startswith(' '):
                # Convert any mixed spaces/tabs to consistent 4-space indentation
                leading_spaces = len(line) - len(line.lstrip())
                if leading_spaces % 4 != 0:
                    normalized_spaces = (leading_spaces // 4) * 4
                    line = ' ' * normalized_spaces + line.lstrip()
                    print(f"DEBUG: Normalized indentation for line: {line.strip()[:50]}...")
            elif line.strip() == 'as pd':
                line = 'import pandas as pd'
                print(f"DEBUG: Fixed incomplete import 'as pd' -> 'import pandas as pd'")
            elif line.strip() == 'as np':
                line = 'import numpy as np'
                print(f"DEBUG: Fixed incomplete import 'as np' -> 'import numpy as np'")
            elif line.strip() == 'as plt':
                line = 'import matplotlib.pyplot as plt'
                print(f"DEBUG: Fixed incomplete import 'as plt' -> 'import matplotlib.pyplot as plt'")
            elif line.strip() == 'as sns':
                line = 'import seaborn as sns'
                print(f"DEBUG: Fixed incomplete import 'as sns' -> 'import seaborn as sns'")
            
            # Replace streamlit import statements with a simple assignment since st is already available
            if line.strip() == 'import streamlit as st' or line.strip() == 'import streamlit':
                line = '# ' + line + '  # st is already available in the environment'
                print(f"DEBUG: Commented out streamlit import: {line.strip()}")
            
            # Fix print statements with trailing colons
            if line.strip().startswith('print(') and line.strip().endswith('):'):
                line = line.rstrip(':')
                print(f"DEBUG: Fixed print statement with trailing colon: {line.strip()}")
            
            # Fix list comprehensions with trailing colons
            if line.strip().endswith(']:') and '[' in line and 'for' in line and 'in' in line:
                line = line.rstrip(':')
                print(f"DEBUG: Fixed list comprehension with trailing colon: {line.strip()}")
            
            # Fix any line ending with colon that shouldn't have one
            if (line.strip().endswith(':') and 
                not any(keyword in line for keyword in ['if ', 'for ', 'while ', 'def ', 'class ', 'try:', 'except:', 'finally:', 'else:', 'elif ', 'with ', 'async def ']) and
                not line.strip().endswith('::') and  # Don't fix slice notation
                not line.strip().endswith('[:') and  # Don't fix slice notation
                not line.strip().endswith(']:')):  # Don't fix list/dict endings
                line = line.rstrip(':')
                print(f"DEBUG: Fixed trailing colon: {line.strip()}")
            
            # Fix unescaped quotes in print statements
            if line.strip().startswith('print(') and '"' in line and line.count('"') % 2 != 0:
                # Count quotes and escape internal ones if needed
                parts = line.split('"')
                if len(parts) > 2:
                    # Escape internal quotes by replacing them with escaped quotes
                    fixed_parts = []
                    for i, part in enumerate(parts):
                        if i == 0:  # Before first quote
                            fixed_parts.append(part)
                        elif i == len(parts) - 1:  # After last quote
                            fixed_parts.append(part)
                        else:  # Between quotes - escape internal quotes
                            fixed_parts.append(part.replace('"', '\\"'))
                    line = '"'.join(fixed_parts)
                    print(f"DEBUG: Fixed unescaped quotes in print statement: {line.strip()}")
            
            # Fix specific problematic quote patterns
            if 'print(' in line and '"data analysis"' in line:
                # Replace the problematic quote with escaped version
                line = line.replace('"data analysis"', '"data analysis"')
                print(f"DEBUG: Fixed specific quote pattern in print statement: {line.strip()}")
            
            # Fix other specific problematic patterns
            if 'print(' in line and '"' in line and line.count('"') % 2 != 0:
                # Try a more aggressive fix for unmatched quotes
                # Find the problematic quote and escape it
                quote_pos = line.find('"', line.find('print(') + 6)  # Skip the opening quote
                if quote_pos != -1:
                    # Find the next quote and escape any quotes in between
                    next_quote_pos = line.find('"', quote_pos + 1)
                    if next_quote_pos != -1:
                        # Escape quotes between the first and second quote
                        before = line[:quote_pos + 1]
                        middle = line[quote_pos + 1:next_quote_pos].replace('"', '\\"')
                        after = line[next_quote_pos:]
                        line = before + middle + after
                        print(f"DEBUG: Fixed aggressive quote escaping: {line.strip()}")
            
                # Fix unmatched parentheses in print statements
    if line.strip().startswith('print(') and line.count('(') != line.count(')'):
        # Count parentheses and fix if needed
        open_count = line.count('(')
        close_count = line.count(')')
        if open_count > close_count:
            # Add missing closing parentheses
            line = line + ')' * (open_count - close_count)
            print(f"DEBUG: Fixed unmatched parentheses in print statement: {line.strip()}")
        elif close_count > open_count:
            # Remove extra closing parentheses
            line = line.rstrip(')') + ')' * open_count
            print(f"DEBUG: Fixed unmatched parentheses in print statement: {line.strip()}")
    
    # Remove markdown code blocks (```)
    if line.strip() == '```':
        line = ''  # Remove the line entirely
        print(f"DEBUG: Removed markdown code block marker")
    elif line.strip().startswith('```'):
        line = line.replace('```', '').strip()  # Remove ``` from the beginning
        print(f"DEBUG: Removed markdown code block start")
    elif line.strip().endswith('```'):
        line = line.replace('```', '').strip()  # Remove ``` from the end
        print(f"DEBUG: Removed markdown code block end")
    
    fixed_lines.append(line)
    
    code = '\n'.join(fixed_lines)
    
    # Try compilation again
    try:
        compile(code, '<string>', 'exec')
        print("DEBUG: Syntax error fixed successfully")
    except SyntaxError as e2:
        print(f"DEBUG: Could not fix syntax error in sanitize_code: {e2}")
        print(f"DEBUG: Error location: line {e2.lineno if hasattr(e2, 'lineno') else 'unknown'}")
        print(f"DEBUG: Error details: {e2.text if hasattr(e2, 'text') else 'unknown'}")
        
        # Try to fix common syntax errors that the line-by-line sanitizer missed
        if "unmatched" in str(e2) and ")" in str(e2):
            print("DEBUG: Attempting to fix unmatched parentheses...")
            # Count total parentheses in the code
        elif "unindent" in str(e2) or "indentation" in str(e2):
            print("DEBUG: Attempting to fix indentation issues...")
            # Normalize indentation to use spaces only
            lines = code.split('\n')
            fixed_lines = []
            for line in lines:
                # Convert tabs to spaces and normalize indentation
                line = line.expandtabs(4)
                # If line has mixed indentation, normalize it
                if line.strip() and line.startswith(' '):
                    # Count leading spaces and normalize to multiples of 4
                    leading_spaces = len(line) - len(line.lstrip())
                    normalized_spaces = (leading_spaces // 4) * 4
                    line = ' ' * normalized_spaces + line.lstrip()
                fixed_lines.append(line)
            code = '\n'.join(fixed_lines)
            print("DEBUG: Normalized indentation to use consistent spaces")
            
            # Try compilation again after indentation fix
            try:
                compile(code, '<string>', 'exec')
                print("DEBUG: Indentation error fixed successfully")
            except SyntaxError as e3:
                print(f"DEBUG: Still has syntax errors after indentation fix: {e3}")
                # Try one more aggressive fix - remove problematic lines
                if "st.dataframe" in str(e3):
                    print("DEBUG: Attempting to fix st.dataframe indentation issues...")
                    lines = code.split('\n')
                    fixed_lines = []
                    for line in lines:
                        # If line contains st.dataframe and has indentation issues, fix it
                        if 'st.dataframe' in line and line.strip() and not line.startswith('#'):
                            # Ensure proper indentation
                            if not line.startswith(' ') and not line.startswith('\t'):
                                # This should be at the top level
                                fixed_lines.append(line)
                            else:
                                # This should be indented - normalize to 4 spaces
                                leading_spaces = len(line) - len(line.lstrip())
                                if leading_spaces % 4 != 0:
                                    normalized_spaces = (leading_spaces // 4) * 4
                                    line = ' ' * normalized_spaces + line.lstrip()
                                fixed_lines.append(line)
                        else:
                            fixed_lines.append(line)
                    code = '\n'.join(fixed_lines)
                    
                    # Try compilation one more time
                    try:
                        compile(code, '<string>', 'exec')
                        print("DEBUG: st.dataframe indentation error fixed successfully")
                    except SyntaxError as e4:
                        print(f"DEBUG: Final syntax error after st.dataframe fix: {e4}")
                        return fallback_code
                else:
                    return fallback_code
            total_open = code.count('(')
            total_close = code.count(')')
            if total_open > total_close:
                code = code + ')' * (total_open - total_close)
                print(f"DEBUG: Added {total_open - total_close} missing closing parentheses")
            elif total_close > total_open:
                # Remove extra closing parentheses from the end
                while code.endswith(')') and total_close > total_open:
                    code = code[:-1]
                    total_close -= 1
                print(f"DEBUG: Removed {total_close - total_open} extra closing parentheses")
            
            # Try compilation again
            try:
                compile(code, '<string>', 'exec')
                print("DEBUG: Syntax error fixed after global parenthesis fix")
            except SyntaxError as e3:
                print(f"DEBUG: Still has syntax errors after global fix: {e3}")
        
        # Don't return fallback here - let the main flow handle LLM fix attempt
        # Just return the code as-is and let the main flow detect the syntax error

    return code


def run_autonomous_session(
    df: pd.DataFrame,
    goal: str,
    time_budget_min: int,
    model: str,
    timeout_seconds: int,
    topic: Optional[str] = None,
    hide_on_error: bool = True,
) -> Dict[str, object]:
    schema, missing_pct, distinct_counts = compute_dataset_info(df)

    # bound LLM codegen time to 20% of budget (min 5s, max budget-10s)
    llm_timeout = max(5, min(int(0.2 * timeout_seconds), max(5, timeout_seconds - 10)))
    
    print(f"DEBUG: Attempting LLM call with timeout {llm_timeout}s")
    try:
        code_text = llm_autonomous_eda_code(
            schema,
            missing_pct,
            distinct_counts,
            goal,
            time_budget_min,
            model=model,
            topic=topic,
            request_timeout=llm_timeout,
        )
        print(f"DEBUG: LLM call successful, response length: {len(code_text)}")
        original_code = extract_code(code_text)
        print(f"DEBUG: Extracted original code length: {len(original_code)}")
        code = sanitize_code(original_code, df, time_budget_min)
        print(f"DEBUG: Sanitized code length: {len(code)}")
    except Exception as e:
        print(f"DEBUG: LLM call failed: {type(e).__name__}: {e}")
        errors = [f"LLM call failed: {type(e).__name__}: {e}"]
        # Fall back to deterministic script immediately
        fb_code = _fallback_script(df, topic, time_budget_min)
        fb_code = sanitize_code(fb_code, df)
        fb_res = execute_code(fb_code, df, timeout_seconds=max(30, timeout_seconds // 2))
        if fb_res.get("error"):
            errors.append(fb_res.get("error") or "")
        sections = parse_sections(fb_res.get("stdout", ""))
        figures = fb_res.get("figures", [])
        if not any(sections.values()) and not figures:
            sections = _last_resort_sections(df)
        returned_error = None if hide_on_error else ("\n\n".join([e for e in errors if e]))
        return {
            "code": f"# LLM call failed: {e}\n\n# ---- FALLBACK (due to LLM failure) ----\n\n{fb_code}",
            "sections": sections,
            "figures": figures,
            "error": returned_error,
            "errors": errors,
        }

    errors: List[str] = []

    # Preflight syntax check: fallback immediately on SyntaxError
    try:
        compile(code, "<string>", "exec")
        print(f"DEBUG: Code compiles successfully")
    except SyntaxError as se:
        print(f"DEBUG: Syntax error in LLM code: {se}")
        errors.append(f"SyntaxError (preflight): {se}")
        
        # Fall back to deterministic script immediately
        fb_code = _fallback_script(df, topic, time_budget_min)
        fb_code = sanitize_code(fb_code, df, time_budget_min)
        fb_res = execute_code(fb_code, df, timeout_seconds=max(30, timeout_seconds // 2))
        if fb_res.get("error"):
            errors.append(fb_res.get("error") or "")
        sections = parse_sections(fb_res.get("stdout", ""))
        figures = fb_res.get("figures", [])
        if not any(sections.values()) and not figures:
            sections = _last_resort_sections(df)
        returned_error = None if hide_on_error else ("\n\n".join([e for e in errors if e]))
        return {
            "code": code + "\n\n# ---- FALLBACK (due to SyntaxError) ----\n\n" + fb_code,
            "sections": sections,
            "figures": figures,
            "error": returned_error,
            "errors": errors,
        }

    # Pre-execution check: ensure no undefined variables
    undefined_vars = []
    for line in code.split('\n'):
        if any(var in line for var in ['topic', 'goal', 'focus', 'context']) and not any(keyword in line for keyword in ['def ', 'import ', 'from ', ' = ', '=']):
            undefined_vars.append(line.strip())
    
    if undefined_vars:
        print(f"DEBUG: Found undefined variables in code, but skipping recursive sanitization to avoid breaking code")
        # Don't apply sanitizer recursively - it's breaking the code
        # Just try to compile as-is
        try:
            compile(code, "<string>", "exec")
            print(f"DEBUG: Code compiles despite undefined variables")
        except SyntaxError as se2:
            print(f"DEBUG: Still has syntax errors: {se2}")
            errors.append(f"SyntaxError: {se2}")
            fb_code = _fallback_script(df, topic, time_budget_min)
            fb_code = sanitize_code(fb_code, df, time_budget_min)
            fb_res = execute_code(fb_code, df, timeout_seconds=max(30, timeout_seconds // 2))
            if fb_res.get("error"):
                errors.append(fb_res.get("error") or "")
            sections = parse_sections(fb_res.get("stdout", ""))
            figures = fb_res.get("figures", [])
            if not any(sections.values()) and not figures:
                sections = _last_resort_sections(df)
            returned_error = None if hide_on_error else ("\n\n".join([e for e in errors if e]))
            return {
                "code": original_code + "\n\n# ---- FALLBACK (due to SyntaxError) ----\n\n" + fb_code,
                "sections": sections,
                "figures": figures,
                "error": returned_error,
                "errors": errors,
            }

    # DEBUG: Print execution info
    print(f"DEBUG: Executing code with timeout {timeout_seconds}s")
    print(f"DEBUG: Code length: {len(code)} characters")
    
    # Set a reasonable timeout for LLM code execution
    grace_factor = 1.5  # No grace factor needed since code runs fast
    timeout_seconds_local = int(timeout_seconds * grace_factor)
    # Cap LLM code execution at 60 seconds for better reliability
    execution_timeout = min(timeout_seconds_local, 60)
    print(f"DEBUG: Using execution timeout: {execution_timeout}s")
    
    # For longer time budgets, allow slightly more execution time
    if time_budget_min >= 10:
        execution_timeout = min(execution_timeout, 90)  # Up to 90 seconds for longer budgets
    elif time_budget_min >= 20:
        execution_timeout = min(execution_timeout, 60)  # Up to 1 minute for 20+ min budgets
    
    result = execute_code(code, df, timeout_seconds=execution_timeout)
    
    # DEBUG: Print execution results
    print(f"DEBUG: Execution completed")
    print(f"DEBUG: stdout length: {len(result.get('stdout', ''))}")
    print(f"DEBUG: figures count: {len(result.get('figures', []))}")
    print(f"DEBUG: error: {result.get('error')}")
    if result.get('error'):
        print(f"DEBUG: FULL ERROR TRACEBACK:")
        print(result.get('error'))
    
    error_msg = result.get("error")
    if error_msg:
        print(f"DEBUG: EXECUTION ERROR DETAILS:")
        print(error_msg)
        errors.append(error_msg)

    use_fallback = False
    if error_msg:
        print(f"DEBUG: Using fallback due to execution error")
        use_fallback = True
    else:
        stdout = result.get("stdout", "") or ""
        figs = result.get("figures", []) or []
        if not stdout.strip() and not figs:
            print(f"DEBUG: Using fallback due to empty output (stdout: {len(stdout)}, figures: {len(figs)})")
            use_fallback = True

    if use_fallback:
        print(f"DEBUG: Using fallback script")
        fb_code = _fallback_script(df, topic, time_budget_min)
        fb_code = sanitize_code(fb_code, df, time_budget_min)
        fb_timeout = max(10, timeout_seconds // 3)
        fb_res = execute_code(fb_code, df, timeout_seconds=fb_timeout)
        if fb_res.get("error"):
            errors.append(fb_res.get("error") or "")
        sections = parse_sections(fb_res.get("stdout", ""))
        figures = fb_res.get("figures", [])
        dataframes = fb_res.get("dataframes", [])  # Get captured dataframes from fallback
        
        # Generate key findings for fallback as well
        if dataframes and not fb_res.get("error"):
            print(f"DEBUG: Generating key findings from fallback data tables")
            try:
                key_findings = llm_generate_key_findings(dataframes, goal, model)
                print(f"DEBUG: Generated fallback key findings: {len(key_findings)} characters")
                
                # Update the Final Explanation section with key findings
                if "Final Explanation" in sections:
                    existing_content = sections["Final Explanation"]
                    if existing_content.strip():
                        # Check if key findings are already present to avoid duplication
                        if "Key Findings:" not in existing_content:
                            sections["Final Explanation"] = f"{existing_content}\n\nKey Findings:\n{key_findings}"
                        else:
                            # Key findings already present, just use existing content
                            sections["Final Explanation"] = existing_content
                    else:
                        sections["Final Explanation"] = f"Key Findings:\n{key_findings}"
                else:
                    sections["Final Explanation"] = f"Key Findings:\n{key_findings}"
                
                # Clean up duplicate sentences in Final Explanation
                if "Final Explanation" in sections:
                    sections["Final Explanation"] = _remove_duplicate_sentences(sections["Final Explanation"])
            except Exception as e:
                print(f"DEBUG: Failed to generate fallback key findings: {e}")
        
        # If still empty, produce minimal last-resort sections locally
        if not any(sections.values()) and not figures:
            sections = _last_resort_sections(df)
        returned_error = None if hide_on_error else ("\n\n".join([e for e in errors if e]))
        return {
            "code": original_code + "\n\n# ---- FALLBACK ----\n\n" + fb_code,
            "sections": sections,
            "figures": figures,
            "error": returned_error,
            "errors": errors,
        }

    # normal path
    print(f"DEBUG: Parsing sections from stdout")
    sections = parse_sections(result.get("stdout", ""))
    figures = result.get("figures", [])
    dataframes = result.get("dataframes", [])  # Get captured dataframes
    
    # DEBUG: Print parsing results
    print(f"DEBUG: Parsed sections:")
    for header, content in sections.items():
        print(f"  {header}: {len(content)} chars")
    
    # Generate key findings based on the data tables
    if dataframes and not error_msg:
        print(f"DEBUG: Generating key findings from {len(dataframes)} data tables")
        try:
            key_findings = llm_generate_key_findings(dataframes, goal, model)
            print(f"DEBUG: Generated key findings: {len(key_findings)} characters")
            
            # Update the Final Explanation section with key findings
            if "Final Explanation" in sections:
                # Combine existing content with key findings, but avoid duplication
                existing_content = sections["Final Explanation"]
                if existing_content.strip():
                    # Check if key findings are already present to avoid duplication
                    if "Key Findings:" not in existing_content:
                        sections["Final Explanation"] = f"{existing_content}\n\nKey Findings:\n{key_findings}"
                    else:
                        # Key findings already present, just use existing content
                        sections["Final Explanation"] = existing_content
                else:
                    sections["Final Explanation"] = f"Key Findings:\n{key_findings}"
            else:
                sections["Final Explanation"] = f"Key Findings:\n{key_findings}"
            
            # Clean up duplicate sentences in Final Explanation
            if "Final Explanation" in sections:
                sections["Final Explanation"] = _remove_duplicate_sentences(sections["Final Explanation"])
        except Exception as e:
            print(f"DEBUG: Failed to generate key findings: {e}")
            # Continue without key findings if generation fails
    
    returned_error = None if hide_on_error else ("\n\n".join([e for e in errors if e]))
    return {
        "code": original_code,
        "sections": sections,
        "figures": figures,
        "dataframes": dataframes,  # Include captured dataframes
        "error": returned_error,
        "errors": errors,
    } 